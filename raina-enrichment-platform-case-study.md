Situation: 
In 2025, Raina underwent a series of re-factors and platform transformations. We went from being a company that only offered a DJ Mixes to offering both mixes and playlists. During this transformation, we realized that we could not build playlists the same way we made mixes. For DJ Mixes, we could curate very tight lists of music and mix them for a cohesive experience. However, if we made playlists from scratch for each and every client, our team would be stretched far too thin and we would be constantly revisiting the same songs to place them in similar playlists.Furthermore, two top 40 songs, while both on the charts at the same time, could be placed in completely different playlists and as such, this variance needed to be accounted for in other factors like energ and how generally accessible/familiar each song was. 

As a result, I set out to find a way to build a system that would allow us to better leverage each & every song within our platform. 

Task: 
In the beginning, I came up with a simple classifier that ran on a series of LLM calls that looked up a particular song, performed a search if necessary, and then returned an energy and an accessibility rating. While this workflow did not "listen" to the song, my hypothesis was that with a human in the loop, we would be able to get to a minimal effective result relatively quickly. My initial goal was not to have the LLM classify the song as accurately as possible but rather to have it get in the right ballpark and then let the human curators carry it across the last mile by correcting any values and assign the correct subgenres to the song. 

For a product surface, I had initially decided on Google Sheets and used a series of AppScripts to coordinate the workflows. My decision here was to optimize for speed and to test my hypothesis as quickly as possible. It proved to be correct and on any initial evaluations, the LLM classifiers were about 60% accurate wtih 75% of results falling within an acceptable deviation. With that, I was able to turn this into a system with a cache that would check every new sheet a curator would process to see if a particular ISRC, a unique identifier code for each sound recording, has been processed, return those results and only in the case of new results, run the workflows. Within a few weeks, our team was using this across multiple sheets. I also proceeded to set up observability in Braintrust to make error analysis and testing prompt variations easier. 

However, the interface sooned to be the limiting factor and as I began to write evals for the classifiers to ensure they did not make, it made doing error analysis & reviewing the curator's work as a means of aligning the evals very difficult. I was taking Hamel Husain and Shreya Shankar's course at the time and one of the big unlocks they talk about is building your own annotation tool. At the same time, I also noticed that our curators, a largely non-technical bunch, were struggling with dealing with the interface and were often switching between screens and interfaces. 

Action: 
Given these circumstances and feedback I had received from the curators, I started off building a simple annotation and tagging tool where we would be able to tag songs, run evals, and actually listen to the tracks all in one place. After drafting the intiial PRD, i Proceeded to prototype the interface in Figma Make and once I was satisfied with how all of the integrations between Spotify for a listening surface and a React front-end worked, I set out to build this with Claude Code and within a week, our team was using the new platform. 

At the same time, Gemini had also improved their grounding on their flash models and I was able to consolidate multiple prompts into one single, larger prompt that first ran a grounding search on the song in question and looked for specific context & reasoning around each song. From there, the LLM would then process it's own classification and tag not only the energy and accessibility but also even suggest the right subgenres to a 78% human alignment. With that, I was able to consolidate our workflows into a single prompt which led to more concurrency and faster processing. 

As time went on, we began to realize that this interface served as the perfect companion to our core product and we could export metadadata in exactly the way our system was expecting which made updating client accounts significantly easier. In addition, with the combination of well-tagged subgenres, accurate energy levels, and the right accessibility scores, we were able to now build "smart" playlists that would consistently update upon those filters which led to us being able to find the greatest point of leverage – playlists that updated on their own whenever music on the platform fit it's criteria. Our curators were no longer re-visiting songs over and over again to see whether they fit into different playlists. Instead, they would be able to specify the criteria needed for the playlist and have the variables to adjust the sound as needed. A surface I am currently experimenting with is adding in an agentic workflow to create curator briefs from client discovery calls and then build out smart playlists directly from them with the curators interacting with an LLM that is orchestrating the filter development. Over time, I can see the use of semantic search across the context further improving this workflow as well. This idea is something I got from Dan Shipper's Agent Native Workflow – link here. 

Over the course of the last few months, I have also setup a few different forms of evals to ensure the outputs stay aligned with our intended goal output. One of these evals/guardrails is a subgenre validator that runs as a hook at the end of the gemini-classification that verifies that the subgenre suggest directly matches one of the subgenres approved for the platform and depending on how close it is, it will either re-run the classification or match it to the closest subgenre with it being flagged for the curator. In addition to this, we run a weekly check against all the most recently uploaded and human-approved songs to test for overall alignment. For this eval, we decided to specifically track how useful the classification was and for a classification to be useful, the energy and accessibility need to be aligned with the human curators selection and one of the subgenres initially suggested by Gemini has to be one of the final subgenres used by the curator. We are currently averaging 87% on this eval and whenever it does dip below 70% for the week, we are notified accordingly to make any adjustments as needed. A point of potential improvement on this point would be to start measuring how "close" the subgenres suggested are to the final curator's selection. For example, if Song A has 3 subgenres suggested by Gemini but the curator only accepts 1, this means that they had to actively decline 2 suggestions. Depending on what those two subgenres suggested were and what our overall alignment metrics are, we could then use an LLM-As-A Judge to determine whether this is a pass or fail. Lastly, another set of evals we could write would be small, code-based evals that ensure a song that mentions reasoning in it's context, is also tagged in a "cover" subgenre. for example, adele's lovesong should be tagged as a refined cover and if the context mentions that this is a cover of lovesong by the cure but fails to then tag it as such, this would cause the eval to fail. I have also noticed that Gemini 3 tends to be very over-eager and sogns that shoudl have one specific subgenre often have 3 suggestions, 1 being spot on but 2 being far reaches that would end up corrupting the integrity of those lists. As such, I may want to revisit the prompt or specifically create an eval to track this to identify any patterns. 

Roadmap and how I would grow the product: 
- introduce more variables like instrumental vs. vocal - I see this as giving even more control over how each list sounds and this dichotomy comes up the most in our discovery calls where operators on the ground stress the importance of balancing vocals with instrumentals. 
- introduce semantic search – In order to do this, I would most likely separate our initial workflow into two different segments with one run specifically in charge of grounding and searching for specific criteria like placement on other playlists, overall sentiment, descriptions, vocals vs instrumental, and other contextual factors. The second run would then be in charge of the actual classification and tagging. This would allow us to then use semantic search to find songs that are similar to the ones we are working with and then use the context from the first run to help guide the second run. Any semantic search then incorporated into other aspects of the product would build on that initial context & reasoning which already has a good deal of structure to it. 
- agent native development - As mentioned above, using Claude's SDK and very effective approach to skills could make this as a very easy process and allow curators to "pair" with an LLM when building playlists. 
- consolidating more core curation workflows in one place  – Something we created to help everyone keep track of their work was a simple kanban board that would help them see which lists they had to work on. I think there's an opportunity to consolidate work across multiple different platforms into this one space. 
- using librosa and essentia to help enhance energy analysis specifically – One area I see us improving our energy analysis in is through actually "listening" to the songs and we could do so with librosa and essentia. However, I would most likely want to wait till a more capable multi-modal model comes along to chain all of this together or potentially hire an ML engineer who has experience building out audio analysis flows before doing this. I initially thought that this would be essential but I've been surprised at how far we have been able to get with LLMs. 
- take the workflow and turn it into an agentic experience that spawns subagents to manage specific processes – I am currently unsure about this but it echoes some of my sentiment that I mentioned earlier where instead of running a workflow for each song, we could build a multi-agent exprience that is able to spawn subagents and use various skills to inform their context around a specific subtask. I suspect we may need more capable reasoning models and we may suffer on latency here but this may be the best way to incorporate self-improvement workflows and remove the curator from the review/metadata curation process entirely. 

Skills Used: 
- AI Prototyping 
- AI Engineering 
- AI Product Design 
- Continous Discovery 
- AI Evaluations 
- Agent-Native Development 